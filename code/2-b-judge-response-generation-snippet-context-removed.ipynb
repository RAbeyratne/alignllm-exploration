{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a9382-1d0f-442b-b8c1-dff14d593012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install datasets\n",
    "!pip install transformers accelerate bitsandbytes\n",
    "!pip install nltk\n",
    "!pip install tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, BertTokenizer, BertModel, AutoTokenizer, AutoModel, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "import seaborn as sns\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e81ee-d229-47d7-bdcd-cc2bfbeeef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_L = \"xxx\"\n",
    "login(token=HF_L)\n",
    "model = 'falcon'\n",
    "\n",
    "if model == 'llama':\n",
    "    llama_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "    llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_name, use_auth_token=HF_L)\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        llama_model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        use_auth_token=HF_L\n",
    "    )\n",
    "if model == 'falcon':\n",
    "    model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "    \n",
    "    falcon_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    falcon_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    falcon_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=falcon_model,\n",
    "        tokenizer=falcon_tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "if model == 'gemma':\n",
    "    gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\n",
    "    gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-7b-it\", \n",
    "        device_map=\"auto\", \n",
    "        revision=\"float16\")\n",
    "if model == 'mistral':\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "    try:\n",
    "        mistral_model = torch.compile(mistral_model)\n",
    "    except Exception:\n",
    "        pass  # ignore if not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acac042a-57a9-4ca3-a515-7523d4b34ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_llama(prompt, max_new_tokens=500, temperature=1, top_k=50, top_p=0.9, seed=42):\n",
    "    inputs = llama_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    torch.manual_seed(seed)\n",
    "    outputs = llama_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    response = llama_tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()\n",
    "    return response\n",
    "\n",
    "def request_falcon(prompt, max_new_tokens=200, temperature=0.7, top_k=50, top_p=0.9):\n",
    "    output = falcon_pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        eos_token_id=falcon_tokenizer.eos_token_id,\n",
    "        pad_token_id=falcon_tokenizer.eos_token_id,\n",
    "        use_cache=False \n",
    "    )\n",
    "    return output[0][\"generated_text\"][len(prompt):].strip()\n",
    "\n",
    "def request_gemma(prompt, max_tokens=500, temperature=1.0, top_k=50, top_p=0.9, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    gemma_model.to(device)\n",
    "    input_ids = gemma_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = gemma_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True\n",
    "        )\n",
    "    generated_text = gemma_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text[len(prompt):].strip()\n",
    "\n",
    "def request_mistral(prompt, max_tokens=500, temperature=1.0, top_k=50, top_p=0.9, seed=42):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    encodeds = mistral_tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(mistral_model.device)\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        generated_ids = mistral_model.generate(\n",
    "            encodeds,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            use_cache=True\n",
    "        )\n",
    "    decoded = mistral_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return decoded[0].strip().replace('[INST] ', '').replace('[/INST] ', '')[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f14cc01-6013-45d4-9c3c-70fef994a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_GENERATION_PROMPT = '''Generate an answer to the below question.\n",
    "\n",
    "question: \"{0}\"\n",
    "'''\n",
    "\n",
    "QUESTION_GENERATION_PROMPT = '''Given the below answer, generate the \"question\" which was likely asked. \n",
    "\n",
    "answer: \"{1}\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de85fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Ramitha/snippet-less-output-all-models-all\")\n",
    "df = pd.DataFrame(dataset['rawcases'])\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if row['model'] == model:\n",
    "        continue;\n",
    "        \n",
    "    question = row['question']\n",
    "    answer = row['answerGenerated']\n",
    "    snippet = row['snippet']\n",
    "\n",
    "    # static temperature for judges\n",
    "    temp = 0.7\n",
    "\n",
    "    prompt = QUESTION_GENERATION_PROMPT.format(snippet, answer)  \n",
    "    if model == 'llama':\n",
    "        response = request_llama(prompt, temperature=temp)\n",
    "    elif model == 'falcon':\n",
    "        response = request_falcon(prompt, temperature=temp)\n",
    "    elif model == 'gemma':\n",
    "        prompt = QUESTION_GENERATION_PROMPT.format(snippet, answer)\n",
    "        response = request_gemma(prompt, temperature=temp)\n",
    "    elif model == 'mistral':\n",
    "        response = request_mistral(prompt, temperature=temp)\n",
    "    df.at[index, 'question_answerGenerated_' + model] = response\n",
    "\n",
    "    prompt = ANSWER_GENERATION_PROMPT.format(response, snippet)  \n",
    "    if model == 'llama':\n",
    "        response_ = request_llama(prompt, temperature=temp)\n",
    "    elif model == 'falcon':\n",
    "        response_ = request_falcon(prompt, temperature=temp)\n",
    "    elif model == 'gemma':\n",
    "        response_ = request_gemma(prompt, temperature=temp)\n",
    "    elif model == 'mistral':\n",
    "        response_ = request_mistral(prompt, temperature=temp)\n",
    "    df.at[index, 'reverse_answer_answerGenerated_' + model] = response_\n",
    "\n",
    "hf_dataset = DatasetDict({\n",
    "    'rawcases': Dataset.from_pandas(df)\n",
    "})\n",
    "hf_dataset.push_to_hub(\"Ramitha/snippet-less-output-all-models-all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
