{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a9382-1d0f-442b-b8c1-dff14d593012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install datasets\n",
    "!pip install transformers accelerate bitsandbytes\n",
    "!pip install nltk\n",
    "!pip install tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "import seaborn as sns\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e81ee-d229-47d7-bdcd-cc2bfbeeef92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2e04c53f7f436e93f509a23279f1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HF_L = \"xxx\"\n",
    "login(token=HF_L)\n",
    "model = 'falcon'\n",
    "\n",
    "if model == 'llama':\n",
    "    llama_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "    llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_name, use_auth_token=HF_L)\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        llama_model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        use_auth_token=HF_L\n",
    "    )\n",
    "if model == 'falcon':\n",
    "    falcon_model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "    falcon_tokenizer = AutoTokenizer.from_pretrained(falcon_model_name)\n",
    "    falcon_model = AutoModelForCausalLM.from_pretrained(\n",
    "        falcon_model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "if model == 'gemma':\n",
    "    gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\n",
    "    gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-7b-it\", \n",
    "        device_map=\"auto\", \n",
    "        revision=\"float16\")\n",
    "if model == 'mistral':\n",
    "    device = \"cuda\"\n",
    "    mistral_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "    mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acac042a-57a9-4ca3-a515-7523d4b34ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama_embeddings(prompt, seed=42):\n",
    "    inputs = llama_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    torch.manual_seed(seed)    \n",
    "    llama_model.config.output_hidden_states = True    \n",
    "    with torch.no_grad():\n",
    "        outputs = llama_model(**inputs)    \n",
    "    embeddings = outputs.hidden_states[-1]\n",
    "    attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "    embeddings = embeddings * attention_mask\n",
    "    sum_embeddings = embeddings.sum(dim=1)\n",
    "    mask_sum = attention_mask.sum(dim=1)\n",
    "    mean_pooled = sum_embeddings / mask_sum\n",
    "    mean_pooled = mean_pooled.float().detach().cpu().numpy()\n",
    "    return mean_pooled[0]\n",
    "    \n",
    "def get_falcon_embeddings(prompt, seed=42):\n",
    "    if not hasattr(falcon_model, \"config\"):\n",
    "        raise ValueError(\"falcon_model is not properly initialized. Please load the model correctly.\")\n",
    "    inputs = falcon_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    torch.manual_seed(seed)\n",
    "    falcon_model.config.output_hidden_states = True\n",
    "    with torch.no_grad():\n",
    "        outputs = falcon_model(**inputs)\n",
    "    embeddings = outputs.hidden_states[-1]\n",
    "    embeddings = outputs.hidden_states[-1]\n",
    "    attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "    embeddings = embeddings * attention_mask\n",
    "    sum_embeddings = embeddings.sum(dim=1)\n",
    "    mask_sum = attention_mask.sum(dim=1)\n",
    "    mean_pooled = sum_embeddings / mask_sum\n",
    "    mean_pooled = mean_pooled.float().detach().cpu().numpy()\n",
    "    return mean_pooled[0]\n",
    "    \n",
    "def get_gemma_embeddings(prompt, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"    \n",
    "    gemma_model.to(device)    \n",
    "    inputs = gemma_tokenizer(prompt, return_tensors=\"pt\")  \n",
    "    input_ids = inputs[\"input_ids\"].to(device)    \n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    gemma_model.config.output_hidden_states = True\n",
    "    with torch.no_grad():\n",
    "        outputs = gemma_model(input_ids=input_ids) \n",
    "    embeddings = outputs.hidden_states[-1]\n",
    "    attention_mask = attention_mask.unsqueeze(-1)\n",
    "    embeddings = embeddings * attention_mask    \n",
    "    sum_embeddings = embeddings.sum(dim=1)\n",
    "    mask_sum = attention_mask.sum(dim=1)    \n",
    "    mean_pooled = sum_embeddings / mask_sum\n",
    "    mean_pooled = mean_pooled.float().detach().cpu().numpy()    \n",
    "    return mean_pooled[0]\n",
    "    \n",
    "def get_mistral_embeddings(prompt, seed=42):\n",
    "    global mistral_model\n",
    "    mistral_model = mistral_model.to(\"cuda\")    \n",
    "    inputs = mistral_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(\"cuda\") for key, value in inputs.items()}    \n",
    "    torch.manual_seed(seed)\n",
    "    mistral_model.config.output_hidden_states = True    \n",
    "    with torch.no_grad():\n",
    "        outputs = mistral_model(**inputs)    \n",
    "    embeddings = outputs.hidden_states[-1]\n",
    "    attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "    embeddings = embeddings * attention_mask\n",
    "    sum_embeddings = embeddings.sum(dim=1)\n",
    "    mask_sum = attention_mask.sum(dim=1)\n",
    "    mean_pooled = sum_embeddings / mask_sum\n",
    "    mean_pooled = mean_pooled.float().detach().cpu().numpy()\n",
    "    return mean_pooled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dee72ccc-c8f1-4749-98f4-f561f80b2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_case_alignment(case_embs, case_base):\n",
    "    try:\n",
    "        emb1, emb2 = case_embs\n",
    "        emb1 = emb1.reshape(1, -1) if emb1.ndim == 1 else emb1\n",
    "        emb2 = emb2.reshape(1, -1) if emb2.ndim == 1 else emb2\n",
    "        alignment_scores = []\n",
    "        for past_case in case_base:\n",
    "            past_prob_emb, past_solution_emb = past_case\n",
    "            past_prob_emb = past_prob_emb.reshape(1, -1) if past_prob_emb.ndim == 1 else past_prob_emb\n",
    "            past_solution_emb = past_solution_emb.reshape(1, -1) if past_solution_emb.ndim == 1 else past_solution_emb\n",
    "            prob_similarity = cosine_similarity(emb1, past_prob_emb)\n",
    "            solution_similarity = cosine_similarity(emb2, past_solution_emb)\n",
    "            alignment_score = (prob_similarity + solution_similarity) / 2.0\n",
    "            alignment_scores.append(alignment_score)\n",
    "        return (sum(alignment_scores) / len(alignment_scores))[0][0]\n",
    "    except:\n",
    "        print('Error')\n",
    "        return 0.0\n",
    "\n",
    "def get_embeddings(model, text):\n",
    "    if model == \"llama\":\n",
    "        return get_llama_embeddings(text)\n",
    "    elif model == \"falcon\":\n",
    "        return get_falcon_embeddings(text)\n",
    "    elif model == \"gemma\":\n",
    "        return get_gemma_embeddings(text)\n",
    "    elif model == \"mistral\":\n",
    "        return get_mistral_embeddings(text)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de85fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Ramitha/unique-records-snippet-combination-all\")\n",
    "df = pd.DataFrame(dataset['rawcases'])\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if model not in [\"llama\", \"falcon\", \"gemma\", \"mistral\"]:\n",
    "        continue\n",
    "    if (row['model'] != model):\n",
    "        continue\n",
    "    q_emb_without_context = np.array(get_embeddings(model, row[\"question\"]))\n",
    "    a_emb_without_context = np.array(get_embeddings(model, row[\"answerGenerated\"]))\n",
    "    q_emb_with_context = np.array(get_embeddings(model, row[\"question\"]  + \" \" + row[\"snippet\"]))\n",
    "    case_without_context = [q_emb_without_context, a_emb_without_context]    \n",
    "    case_with_problem_context = [q_emb_with_context, a_emb_without_context]\n",
    "    \n",
    "    case_base_without_context, case_base_with_context, case_base_with_problem_context, case_base_with_answer_context = [], [], [], []\n",
    "    for other_model in [\"llama\", \"falcon\", \"gemma\", \"mistral\"]:\n",
    "        if other_model == model:\n",
    "            continue\n",
    "        q_wo = np.array(get_embeddings(model, row[f\"question_answerGenerated_{other_model}\"]))\n",
    "        q_wc = np.array(get_embeddings(model, row[f\"question_answerGenerated_{other_model}\"] + \" \" + row[\"snippet\"]))\n",
    "        a_wo = np.array(get_embeddings(model, row[f\"reverse_answer_answerGenerated_{other_model}\"]))\n",
    "        case_base_without_context.append([q_wo, a_wo])\n",
    "        case_base_with_problem_context.append([q_wc, a_wo])\n",
    "    df.at[index, f\"ILRAlign_{model}\"] = get_case_alignment(case_without_context, case_base_without_context)\n",
    "    df.at[index, f\"ILRAlign_with_problem_context_only_{model}\"] = get_case_alignment(case_with_problem_context, case_base_with_problem_context)\n",
    "\n",
    "hf_dataset = DatasetDict({\n",
    "    'rawcases': Dataset.from_pandas(df)\n",
    "})\n",
    "hf_dataset.push_to_hub(\"Ramitha/unique-records-snippet-combination-all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
