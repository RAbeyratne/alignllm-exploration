{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a9382-1d0f-442b-b8c1-dff14d593012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install datasets\n",
    "!pip install transformers accelerate bitsandbytes\n",
    "!pip install nltk\n",
    "!pip install tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "import seaborn as sns\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e81ee-d229-47d7-bdcd-cc2bfbeeef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_L = \"xxx\"\n",
    "login(token=HF_L)\n",
    "model = 'mistral'\n",
    "\n",
    "if model == 'llama':\n",
    "    llama_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "    llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_name, use_auth_token=HF_L)\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        llama_model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        use_auth_token=HF_L\n",
    "    )\n",
    "if model == 'falcon':\n",
    "    falcon_model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "    falcon_tokenizer = AutoTokenizer.from_pretrained(falcon_model_name)\n",
    "    falcon_model = AutoModelForCausalLM.from_pretrained(\n",
    "        falcon_model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "if model == 'gemma':\n",
    "    gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\n",
    "    gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-7b-it\", \n",
    "        device_map=\"auto\", \n",
    "        revision=\"float16\")\n",
    "if model == 'mistral':\n",
    "    device = \"cuda\"\n",
    "    mistral_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "    mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acac042a-57a9-4ca3-a515-7523d4b34ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_llama(prompt, max_new_tokens=500, temperature=1, top_k=50, top_p=0.9, seed=42):\n",
    "    inputs = llama_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    torch.manual_seed(seed)\n",
    "    outputs = llama_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    response = llama_tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()\n",
    "    return response\n",
    "\n",
    "def request_falcon(prompt, max_new_tokens=500, temperature=1, top_k=50, top_p=0.9, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    inputs = falcon_tokenizer(prompt, return_tensors=\"pt\").to(falcon_model.device)\n",
    "    input_length = inputs[\"input_ids\"].shape[1]  \n",
    "    outputs = falcon_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        eos_token_id=falcon_tokenizer.eos_token_id,\n",
    "        pad_token_id=falcon_tokenizer.eos_token_id,\n",
    "    )\n",
    "    generated_tokens = outputs[0][input_length:]    \n",
    "    return falcon_tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "def request_gemma(prompt, max_tokens=500, temperature=1.0, top_k=50, top_p=0.9, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    gemma_model.to(device)\n",
    "    input_ids = gemma_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = gemma_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True\n",
    "        )\n",
    "    generated_text = gemma_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text[len(prompt):].strip()\n",
    "\n",
    "def request_mistral(prompt, max_tokens=500, temperature=1.0, top_k=50, top_p=0.9, seed=42):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    mistral_model.to(device)\n",
    "    encodeds = mistral_tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "    model_inputs = encodeds.to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = mistral_model.generate(\n",
    "            model_inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True\n",
    "        )\n",
    "    decoded = mistral_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return decoded[0].strip().replace('[INST] ', '').replace('[/INST] ', '')[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f14cc01-6013-45d4-9c3c-70fef994a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_GENERATION_PROMPT = '''Generate an answer to the below question based on the provided snippet.\n",
    "\n",
    "question: \"{0}\"\n",
    "snippet: \"{1}\"\n",
    "'''\n",
    "\n",
    "QUESTION_GENERATION_PROMPT = '''Your task is to generate a clear and concise question based on the provided snippet and answer. Ensure that the generated question directly corresponds to the snippet's content and leads to the given answer.\n",
    "\n",
    "Here is the input:\n",
    "Snippet: \"{0}\"\n",
    "Answer: \"{1}\"\n",
    "\n",
    "Generate the most appropriate question:'''\n",
    "\n",
    "QUESTION_GENERATION_PROMPT_GEMMA = '''Generate the question which was asked regarding the below snippet and provided answer. Ensure that the generated question directly corresponds to the snippet's content and leads to the given answer.\n",
    "\n",
    "snippet: \"{0}\"\n",
    "answer: \"{1}\"\n",
    "\n",
    "The output should contain only the question (don't output the answer to the question).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de85fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Ramitha/unique-records-snippet-combination\")\n",
    "df = pd.DataFrame(dataset['rawcases'])\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if row['model'] == model:\n",
    "        continue;\n",
    "        \n",
    "    question = row['question']\n",
    "    answer = row['answerGenerated']\n",
    "    snippet = row['snippet']\n",
    "\n",
    "    # static temperature for judges\n",
    "    temp = 0.7\n",
    "\n",
    "    prompt = QUESTION_GENERATION_PROMPT.format(snippet, answer)  \n",
    "    if model == 'llama':\n",
    "        response = request_llama(prompt, temperature=temp)\n",
    "    elif model == 'falcon':\n",
    "        response = request_falcon(prompt, temperature=temp)\n",
    "    elif model == 'gemma':\n",
    "        prompt = QUESTION_GENERATION_PROMPT_GEMMA.format(snippet, answer)\n",
    "        response = request_gemma(prompt, temperature=temp)\n",
    "    elif model == 'mistral':\n",
    "        response = request_mistral(prompt, temperature=temp)\n",
    "    df.at[index, 'question_answerGenerated_' + model] = response\n",
    "\n",
    "    prompt = ANSWER_GENERATION_PROMPT.format(response, snippet)  \n",
    "    if model == 'llama':\n",
    "        response_ = request_llama(prompt, temperature=temp)\n",
    "    elif model == 'falcon':\n",
    "        response_ = request_falcon(prompt, temperature=temp)\n",
    "    elif model == 'gemma':\n",
    "        response_ = request_gemma(prompt, temperature=temp)\n",
    "    elif model == 'mistral':\n",
    "        response_ = request_mistral(prompt, temperature=temp)\n",
    "    df.at[index, 'reverse_answer_answerGenerated_' + model] = response_\n",
    "\n",
    "hf_dataset = DatasetDict({\n",
    "    'rawcases': Dataset.from_pandas(df)\n",
    "})\n",
    "hf_dataset.push_to_hub(\"Ramitha/unique-records-snippet-combination\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
